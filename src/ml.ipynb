{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.constants import SEED, TEST_DIR, TRAIN_DIR\n",
    "from modules.data_loader import DataLoader\n",
    "from modules.preprocessor import Preprocessor\n",
    "from modules.vectorizer import Vectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from typing import Callable, List, Tuple\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following notebook, various machine learning techniques applied to the context of text classification are analyzed. Specifically, the IMDB dataset is used, which includes a split into training data (25k) and test data (25k). The exploratory notebook contains exploratory analysis of the dataset. Below are the techniques applied:\n",
    "- **Linear SVM**: Support Vector Machine with a `linear kernel`, where tuning of the hyperparameter `C` and the parameter `ngram_range` will be performed\n",
    "- **Naive Bayes**: Naive Bayes with tuning of the hyperparameter `alpha` and the parameter `ngram_range`\n",
    "- **Logistic Regression**: ogistic Regression with tuning of the hyperparameter `C` and the parameter `ngram_range`\n",
    "\n",
    "These techniques will be applied to the balanced training dataset and preprocessed using the `perform_strong_preprocessing` function of the `Preprocessor` class. This preprocessing technique involves the removal of `HTML tags`, `punctuation`, and `English stop words`. Subsequently, an analysis will be conducted on the use of preprocessing techniques as they do not always yield the best results.\n",
    "\n",
    "In conclusion, the obtained results will be presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(seed = SEED)\n",
    "preprocessor = Preprocessor(stopwords_language = 'english')\n",
    "vectorizer = Vectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = data_loader.load_test_data(TRAIN_DIR)\n",
    "X_test, y_test = data_loader.load_test_data(TEST_DIR)\n",
    "\n",
    "X_train, y_train = [review.decode() for review in X_train], y_train\n",
    "X_test, y_test = [review.decode() for review in X_test], y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(\n",
    "\t\tX_train: List[str],\n",
    "\t\ty_train: List[int],\n",
    "\t\tX_test: List[str],\n",
    "\t\ty_test: List[int],\n",
    "\t\tmodel_name: str = 'LinearSVC',\n",
    "\t\toutput_mode: str = 'tf-idf',\n",
    "\t\tmax_features: int = None,\n",
    "\t\tpreprocessor_fnc: Callable = None,\n",
    "\t\tngram_range: Tuple[int, int] = (1, 1),\n",
    "\t\t**kwargs\n",
    "\t):\n",
    "\n",
    "\tassert model_name in ['LinearSVC', 'LogisticRegression', 'MultinomialNB'], 'Invalid model name'\n",
    "\n",
    "\tvectorize, train_features = vectorizer.vectorize_data(\n",
    "\t\tX_train,\n",
    "\t\toutput_mode,\n",
    "\t\tmax_features,\n",
    "\t\tngram_range,\n",
    "\t\tpreprocessor_fnc,\n",
    "\t)\n",
    "\ttest_features = vectorize.transform(X_test)\n",
    "\n",
    "\tif model_name == 'LinearSVC':\n",
    "\t\tmodel = LinearSVC(C = kwargs['C'], dual = True, max_iter = 5000)\n",
    "\telif model_name == 'MultinomialNB':\n",
    "\t\tmodel = MultinomialNB(alpha = kwargs['alpha'])\n",
    "\telse:\n",
    "\t\tmodel = LogisticRegression(penalty = kwargs['penalty'], C = kwargs['C'])\n",
    "\n",
    "\tmodel.fit(train_features, y_train)\n",
    "\tpredictions = model.predict(test_features)\n",
    "\n",
    "\treturn predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning model with hyper/parameters: {'C': 0.1, 'ngram_range': (1, 1)} - Accuracy: 0.8838\n",
      "Fine tuning model with hyper/parameters: {'C': 0.1, 'ngram_range': (1, 2)} - Accuracy: 0.88316\n",
      "Fine tuning model with hyper/parameters: {'C': 0.1, 'ngram_range': (1, 3)} - Accuracy: 0.87464\n",
      "Fine tuning model with hyper/parameters: {'C': 0.1, 'ngram_range': (2, 2)} - Accuracy: 0.84048\n",
      "Fine tuning model with hyper/parameters: {'C': 0.1, 'ngram_range': (2, 3)} - Accuracy: 0.83676\n",
      "Fine tuning model with hyper/parameters: {'C': 0.1, 'ngram_range': (3, 3)} - Accuracy: 0.7402\n",
      "Fine tuning model with hyper/parameters: {'C': 1, 'ngram_range': (1, 1)} - Accuracy: 0.8702\n",
      "Fine tuning model with hyper/parameters: {'C': 1, 'ngram_range': (1, 2)} - Accuracy: 0.892\n",
      "Fine tuning model with hyper/parameters: {'C': 1, 'ngram_range': (1, 3)} - Accuracy: 0.89096\n",
      "Fine tuning model with hyper/parameters: {'C': 1, 'ngram_range': (2, 2)} - Accuracy: 0.85072\n",
      "Fine tuning model with hyper/parameters: {'C': 1, 'ngram_range': (2, 3)} - Accuracy: 0.84748\n",
      "Fine tuning model with hyper/parameters: {'C': 1, 'ngram_range': (3, 3)} - Accuracy: 0.74088\n",
      "Fine tuning model with hyper/parameters: {'C': 10, 'ngram_range': (1, 1)} - Accuracy: 0.84812\n",
      "Fine tuning model with hyper/parameters: {'C': 10, 'ngram_range': (1, 2)} - Accuracy: 0.8914\n",
      "Fine tuning model with hyper/parameters: {'C': 10, 'ngram_range': (1, 3)} - Accuracy: 0.8926\n",
      "Fine tuning model with hyper/parameters: {'C': 10, 'ngram_range': (2, 2)} - Accuracy: 0.85108\n",
      "Fine tuning model with hyper/parameters: {'C': 10, 'ngram_range': (2, 3)} - Accuracy: 0.84992\n",
      "Fine tuning model with hyper/parameters: {'C': 10, 'ngram_range': (3, 3)} - Accuracy: 0.7418\n",
      "Best parameters: {'C': 10, 'ngram_range': (1, 3)} with accuracy: 0.8926\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "\t'ngram_range': [(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)],\n",
    "\t'C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "grid = list(ParameterGrid(params))\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for param in grid:\n",
    "\tpredictions = create_model(\n",
    "\t\tX_train,\n",
    "\t\ty_train,\n",
    "\t\tX_test,\n",
    "\t\ty_test,\n",
    "\t\tmodel_name = 'LinearSVC',\n",
    "\t\tpreprocessor_fnc = preprocessor.perform_strong_preprocessing,\n",
    "\t\t**param\n",
    "\t)\n",
    "\taccuracy = accuracy_score(y_test, predictions)\n",
    "\tprint(f'Fine tuning model with hyper/parameters: {param} - Accuracy: {accuracy}')\n",
    "\taccuracies.append(accuracy)\n",
    "\n",
    "index = np.argmax(np.array(accuracies))\n",
    "best_params = grid[index]\n",
    "print(f'Best parameters: {best_params} with accuracy: {accuracies[index]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model achieving the best accuracy of `0.8926` utilizes the parameters: `C = 10`, `ngram_range = (1, 3)`. In general, from the results, we can infer that the use of n-grams with ranges `(1, 2)` and `(1, 3)` enables the training of models with higher accuracy (as hypothesized during the exploration phase)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning model with hyper/parameters: {'alpha': 0.1, 'ngram_range': (1, 1)} - Accuracy: 0.81352\n",
      "Fine tuning model with hyper/parameters: {'alpha': 0.1, 'ngram_range': (1, 2)} - Accuracy: 0.858\n",
      "Fine tuning model with hyper/parameters: {'alpha': 0.1, 'ngram_range': (1, 3)} - Accuracy: 0.86188\n",
      "Fine tuning model with hyper/parameters: {'alpha': 0.1, 'ngram_range': (2, 2)} - Accuracy: 0.84932\n",
      "Fine tuning model with hyper/parameters: {'alpha': 0.1, 'ngram_range': (2, 3)} - Accuracy: 0.8558\n",
      "Fine tuning model with hyper/parameters: {'alpha': 0.1, 'ngram_range': (3, 3)} - Accuracy: 0.7422\n",
      "Fine tuning model with hyper/parameters: {'alpha': 1, 'ngram_range': (1, 1)} - Accuracy: 0.83536\n",
      "Fine tuning model with hyper/parameters: {'alpha': 1, 'ngram_range': (1, 2)} - Accuracy: 0.86412\n",
      "Fine tuning model with hyper/parameters: {'alpha': 1, 'ngram_range': (1, 3)} - Accuracy: 0.86588\n",
      "Fine tuning model with hyper/parameters: {'alpha': 1, 'ngram_range': (2, 2)} - Accuracy: 0.85636\n",
      "Fine tuning model with hyper/parameters: {'alpha': 1, 'ngram_range': (2, 3)} - Accuracy: 0.85468\n",
      "Fine tuning model with hyper/parameters: {'alpha': 1, 'ngram_range': (3, 3)} - Accuracy: 0.74132\n",
      "Fine tuning model with hyper/parameters: {'alpha': 10, 'ngram_range': (1, 1)} - Accuracy: 0.8438\n",
      "Fine tuning model with hyper/parameters: {'alpha': 10, 'ngram_range': (1, 2)} - Accuracy: 0.85908\n",
      "Fine tuning model with hyper/parameters: {'alpha': 10, 'ngram_range': (1, 3)} - Accuracy: 0.85952\n",
      "Fine tuning model with hyper/parameters: {'alpha': 10, 'ngram_range': (2, 2)} - Accuracy: 0.83796\n",
      "Fine tuning model with hyper/parameters: {'alpha': 10, 'ngram_range': (2, 3)} - Accuracy: 0.83528\n",
      "Fine tuning model with hyper/parameters: {'alpha': 10, 'ngram_range': (3, 3)} - Accuracy: 0.73404\n",
      "Best parameters: {'alpha': 1, 'ngram_range': (1, 3)} with accuracy: 0.86588\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "\t'ngram_range': [(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)],\n",
    "\t'alpha': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "grid = list(ParameterGrid(params))\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for param in grid:\n",
    "\tpredictions = create_model(\n",
    "\t\tX_train,\n",
    "\t\ty_train,\n",
    "\t\tX_test,\n",
    "\t\ty_test,\n",
    "\t\tmodel_name = 'MultinomialNB',\n",
    "\t\tpreprocessor_fnc = preprocessor.perform_strong_preprocessing,\n",
    "\t\t**param\n",
    "\t)\n",
    "\taccuracy = accuracy_score(y_test, predictions)\n",
    "\tprint(f'Fine tuning model with hyper/parameters: {param} - Accuracy: {accuracy}')\n",
    "\taccuracies.append(accuracy)\n",
    "\n",
    "index = np.argmax(np.array(accuracies))\n",
    "best_params = grid[index]\n",
    "print(f'Best parameters: {best_params} with accuracy: {accuracies[index]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model achieving the best accuracy of `0.86588` utilizes the parameters: `alpha = 1`, `ngram_range = (1, 3)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning model with hyper/parameters: {'C': 0.1, 'ngram_range': (1, 1), 'penalty': 'l2'} - Accuracy: 0.86188\n",
      "Fine tuning model with hyper/parameters: {'C': 0.1, 'ngram_range': (1, 2), 'penalty': 'l2'} - Accuracy: 0.85232\n",
      "Fine tuning model with hyper/parameters: {'C': 0.1, 'ngram_range': (1, 3), 'penalty': 'l2'} - Accuracy: 0.84644\n",
      "Fine tuning model with hyper/parameters: {'C': 0.1, 'ngram_range': (2, 2), 'penalty': 'l2'} - Accuracy: 0.82248\n",
      "Fine tuning model with hyper/parameters: {'C': 0.1, 'ngram_range': (2, 3), 'penalty': 'l2'} - Accuracy: 0.82504\n",
      "Fine tuning model with hyper/parameters: {'C': 0.1, 'ngram_range': (3, 3), 'penalty': 'l2'} - Accuracy: 0.74024\n",
      "Fine tuning model with hyper/parameters: {'C': 1, 'ngram_range': (1, 1), 'penalty': 'l2'} - Accuracy: 0.88304\n",
      "Fine tuning model with hyper/parameters: {'C': 1, 'ngram_range': (1, 2), 'penalty': 'l2'} - Accuracy: 0.88164\n",
      "Fine tuning model with hyper/parameters: {'C': 1, 'ngram_range': (1, 3), 'penalty': 'l2'} - Accuracy: 0.87448\n",
      "Fine tuning model with hyper/parameters: {'C': 1, 'ngram_range': (2, 2), 'penalty': 'l2'} - Accuracy: 0.84248\n",
      "Fine tuning model with hyper/parameters: {'C': 1, 'ngram_range': (2, 3), 'penalty': 'l2'} - Accuracy: 0.83812\n",
      "Fine tuning model with hyper/parameters: {'C': 1, 'ngram_range': (3, 3), 'penalty': 'l2'} - Accuracy: 0.74\n",
      "Fine tuning model with hyper/parameters: {'C': 10, 'ngram_range': (1, 1), 'penalty': 'l2'} - Accuracy: 0.87584\n",
      "Fine tuning model with hyper/parameters: {'C': 10, 'ngram_range': (1, 2), 'penalty': 'l2'} - Accuracy: 0.8908\n",
      "Fine tuning model with hyper/parameters: {'C': 10, 'ngram_range': (1, 3), 'penalty': 'l2'} - Accuracy: 0.88756\n",
      "Fine tuning model with hyper/parameters: {'C': 10, 'ngram_range': (2, 2), 'penalty': 'l2'} - Accuracy: 0.84896\n",
      "Fine tuning model with hyper/parameters: {'C': 10, 'ngram_range': (2, 3), 'penalty': 'l2'} - Accuracy: 0.8456\n",
      "Fine tuning model with hyper/parameters: {'C': 10, 'ngram_range': (3, 3), 'penalty': 'l2'} - Accuracy: 0.74044\n",
      "Best parameters: {'C': 10, 'ngram_range': (1, 2), 'penalty': 'l2'} with accuracy: 0.8908\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "\t'ngram_range': [(1, 1), (1, 2), (1, 3), (2, 2), (2, 3), (3, 3)],\n",
    "\t'penalty': ['l2'],\n",
    "\t'C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "grid = list(ParameterGrid(params))\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for param in grid:\n",
    "\tpredictions = create_model(\n",
    "\t\tX_train,\n",
    "\t\ty_train,\n",
    "\t\tX_test,\n",
    "\t\ty_test,\n",
    "\t\tmodel_name = 'LogisticRegression',\n",
    "\t\tpreprocessor_fnc = preprocessor.perform_strong_preprocessing,\n",
    "\t\t**param\n",
    "\t)\n",
    "\taccuracy = accuracy_score(y_test, predictions)\n",
    "\tprint(f'Fine tuning model with hyper/parameters: {param} - Accuracy: {accuracy}')\n",
    "\taccuracies.append(accuracy)\n",
    "\n",
    "index = np.argmax(np.array(accuracies))\n",
    "best_params = grid[index]\n",
    "print(f'Best parameters: {best_params} with accuracy: {accuracies[index]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model achieving the best accuracy of `0.8908` utilizes the parameters: `C = 10`, `ngram_range = (1, 2)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Does preprocessing have a positive impact?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we analyze the influence of preprocessing on the best models obtained previously:\n",
    "- **Linear SVC**: C = 10, ngram_range = (1, 3)\n",
    "- **Naive Bayes**: alpha = 1, ngram_range = (1, 3)\n",
    "- **Logistic Regression**: C = 10, ngram_range = (1, 2)\n",
    "\n",
    "In text classification problems, preprocessing techniques are often used to remove data that is not useful for training the model. There is no certainty that removing these steps results in more accurate models. Therefore, we analyze three scenarios on the models with the highest accuracy obtained previously:\n",
    "- **perform_strong_preprocessing**: html tags + punctuation + stop words (English)\n",
    "- **perform_soft_preprocessing**: html tags + punctuation\n",
    "- **without any preprocessing step**: -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning model with hyper/parameters: {'C': 10, 'ngram_range': (1, 3), 'preprocessor_fnc': <bound method Preprocessor.perform_strong_preprocessing of <modules.preprocessor.Preprocessor object at 0x291f80b50>>} - Accuracy: 0.8926\n",
      "Fine tuning model with hyper/parameters: {'C': 10, 'ngram_range': (1, 3), 'preprocessor_fnc': <bound method Preprocessor.perform_soft_preprocessing of <modules.preprocessor.Preprocessor object at 0x291f80b50>>} - Accuracy: 0.90428\n",
      "Fine tuning model with hyper/parameters: {'C': 10, 'ngram_range': (1, 3), 'preprocessor_fnc': None} - Accuracy: 0.90492\n",
      "Best parameters: {'C': 10, 'ngram_range': (1, 3), 'preprocessor_fnc': None} with accuracy: 0.90492\n",
      "Accuracy: 0.90492, F1: 0.9053629016204164, Precision: 0.9011650947134818, Recall: 0.9096\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.90      0.90     12500\n",
      "           1       0.90      0.91      0.91     12500\n",
      "\n",
      "    accuracy                           0.90     25000\n",
      "   macro avg       0.90      0.90      0.90     25000\n",
      "weighted avg       0.90      0.90      0.90     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "\t'preprocessor_fnc': [preprocessor.perform_strong_preprocessing, preprocessor.perform_soft_preprocessing, None],\n",
    "\t'ngram_range': [(1, 3)],\n",
    "\t'C': [10]\n",
    "}\n",
    "\n",
    "grid = list(ParameterGrid(params))\n",
    "\n",
    "accuracies = []\n",
    "predictions_list = []\n",
    "\n",
    "for param in grid:\n",
    "\tpredictions = create_model(\n",
    "\t\tX_train,\n",
    "\t\ty_train,\n",
    "\t\tX_test,\n",
    "\t\ty_test,\n",
    "\t\tmodel_name = 'LinearSVC',\n",
    "\t\t**param\n",
    "\t)\n",
    "\taccuracy = accuracy_score(y_test, predictions)\n",
    "\tprint(f'Fine tuning model with hyper/parameters: {param} - Accuracy: {accuracy}')\n",
    "\taccuracies.append(accuracy)\n",
    "\tpredictions_list.append(predictions)\n",
    "\n",
    "index = np.argmax(np.array(accuracies))\n",
    "best_params = grid[index]\n",
    "print(f'Best parameters: {best_params} with accuracy: {accuracies[index]}')\n",
    "print(f'Accuracy: {accuracy_score(y_test, predictions_list[index])}, F1: {f1_score(y_test, predictions_list[index])}, Precision: {precision_score(y_test, predictions_list[index])}, Recall: {recall_score(y_test, predictions_list[index])}')\n",
    "print(classification_report(y_test, predictions_list[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the obtained results, it can be observed that the accuracy reaches `0.9049` without any preprocessing technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning model with hyper/parameters: {'alpha': 1, 'ngram_range': (1, 3), 'preprocessor_fnc': <bound method Preprocessor.perform_strong_preprocessing of <modules.preprocessor.Preprocessor object at 0x291f80b50>>} - Accuracy: 0.86588\n",
      "Fine tuning model with hyper/parameters: {'alpha': 1, 'ngram_range': (1, 3), 'preprocessor_fnc': <bound method Preprocessor.perform_soft_preprocessing of <modules.preprocessor.Preprocessor object at 0x291f80b50>>} - Accuracy: 0.87664\n",
      "Fine tuning model with hyper/parameters: {'alpha': 1, 'ngram_range': (1, 3), 'preprocessor_fnc': None} - Accuracy: 0.87684\n",
      "Best parameters: {'alpha': 1, 'ngram_range': (1, 3), 'preprocessor_fnc': None} with accuracy: 0.87684\n",
      "Accuracy: 0.87684, F1: 0.871980375036381, Precision: 0.9078001904597005, Recall: 0.83888\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.91      0.88     12500\n",
      "           1       0.91      0.84      0.87     12500\n",
      "\n",
      "    accuracy                           0.88     25000\n",
      "   macro avg       0.88      0.88      0.88     25000\n",
      "weighted avg       0.88      0.88      0.88     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "\t'preprocessor_fnc': [preprocessor.perform_strong_preprocessing, preprocessor.perform_soft_preprocessing, None],\n",
    "\t'ngram_range': [(1, 3)],\n",
    "\t'alpha': [1]\n",
    "}\n",
    "\n",
    "grid = list(ParameterGrid(params))\n",
    "\n",
    "accuracies = []\n",
    "predictions_list = []\n",
    "\n",
    "for param in grid:\n",
    "\tpredictions = create_model(\n",
    "\t\tX_train,\n",
    "\t\ty_train,\n",
    "\t\tX_test,\n",
    "\t\ty_test,\n",
    "\t\tmodel_name = 'MultinomialNB',\n",
    "\t\t**param\n",
    "\t)\n",
    "\taccuracy = accuracy_score(y_test, predictions)\n",
    "\tprint(f'Fine tuning model with hyper/parameters: {param} - Accuracy: {accuracy}')\n",
    "\taccuracies.append(accuracy)\n",
    "\tpredictions_list.append(predictions)\n",
    "\n",
    "index = np.argmax(np.array(accuracies))\n",
    "best_params = grid[index]\n",
    "print(f'Best parameters: {best_params} with accuracy: {accuracies[index]}')\n",
    "print(f'Accuracy: {accuracy_score(y_test, predictions_list[index])}, F1: {f1_score(y_test, predictions_list[index])}, Precision: {precision_score(y_test, predictions_list[index])}, Recall: {recall_score(y_test, predictions_list[index])}')\n",
    "print(classification_report(y_test, predictions_list[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the obtained results, it can be observed that the accuracy reaches `0.8768` without any preprocessing technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning model with hyper/parameters: {'C': 10, 'ngram_range': (1, 2), 'penalty': 'l2', 'preprocessor_fnc': <bound method Preprocessor.perform_strong_preprocessing of <modules.preprocessor.Preprocessor object at 0x291f80b50>>} - Accuracy: 0.8908\n",
      "Fine tuning model with hyper/parameters: {'C': 10, 'ngram_range': (1, 2), 'penalty': 'l2', 'preprocessor_fnc': <bound method Preprocessor.perform_soft_preprocessing of <modules.preprocessor.Preprocessor object at 0x291f80b50>>} - Accuracy: 0.90112\n",
      "Fine tuning model with hyper/parameters: {'C': 10, 'ngram_range': (1, 2), 'penalty': 'l2', 'preprocessor_fnc': None} - Accuracy: 0.90088\n",
      "Best parameters: {'C': 10, 'ngram_range': (1, 2), 'penalty': 'l2', 'preprocessor_fnc': <bound method Preprocessor.perform_soft_preprocessing of <modules.preprocessor.Preprocessor object at 0x291f80b50>>} with accuracy: 0.90112\n",
      "Accuracy: 0.90112, F1: 0.9014825442372071, Precision: 0.8981893265565438, Recall: 0.9048\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90     12500\n",
      "           1       0.90      0.90      0.90     12500\n",
      "\n",
      "    accuracy                           0.90     25000\n",
      "   macro avg       0.90      0.90      0.90     25000\n",
      "weighted avg       0.90      0.90      0.90     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "\t'preprocessor_fnc': [preprocessor.perform_strong_preprocessing, preprocessor.perform_soft_preprocessing, None],\n",
    "\t'ngram_range': [(1, 2)],\n",
    "\t'penalty': ['l2'],\n",
    "\t'C': [10]\n",
    "}\n",
    "\n",
    "grid = list(ParameterGrid(params))\n",
    "\n",
    "accuracies = []\n",
    "predictions_list = []\n",
    "\n",
    "for param in grid:\n",
    "\tpredictions = create_model(\n",
    "\t\tX_train,\n",
    "\t\ty_train,\n",
    "\t\tX_test,\n",
    "\t\ty_test,\n",
    "\t\tmodel_name = 'LogisticRegression',\n",
    "\t\t**param\n",
    "\t)\n",
    "\taccuracy = accuracy_score(y_test, predictions)\n",
    "\tprint(f'Fine tuning model with hyper/parameters: {param} - Accuracy: {accuracy}')\n",
    "\taccuracies.append(accuracy)\n",
    "\tpredictions_list.append(predictions)\n",
    "\n",
    "index = np.argmax(np.array(accuracies))\n",
    "best_params = grid[index]\n",
    "print(f'Best parameters: {best_params} with accuracy: {accuracies[index]}')\n",
    "print(f'Accuracy: {accuracy_score(y_test, predictions_list[index])}, F1: {f1_score(y_test, predictions_list[index])}, Precision: {precision_score(y_test, predictions_list[index])}, Recall: {recall_score(y_test, predictions_list[index])}')\n",
    "print(classification_report(y_test, predictions_list[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the obtained results, it can be observed that the accuracy reaches `0.9011` without any preprocessing technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Below are the tables containing the metrics of the models with the highest accuracy obtained from the various methodologies.\n",
    "\n",
    "| Model  | Accuracy | F1-Score | Recall | Precision |\n",
    "| ------------- | ------------- | -------------  | -------------  | -------------  |\n",
    "| Support Vector Classifier  | <u>**0.9049**</u> | <u>**0.9053**</u> | 0.9011 | <u>**0.9096**</u> |\n",
    "| Naive Bayes  | 0.8768 | 0.8719 | <u>**0.9078**</u> | 0.8388 |\n",
    "| Logistic Regression  | 9011 | 0.9014 | 0.8981 | 0.9048 |\n",
    "\n",
    "In addition, the table containing the different results obtained using different preprocessing techniques is reported.\n",
    "\n",
    "| Model  | Strong Preprocessing | Soft Preprocessing | No Preprocessing |\n",
    "| ------------- | ------------- | -------------  | -------------  |\n",
    "| Support Vector Classifier  | 0.8926 | 0.9042 | <u>**0.9049**</u> |\n",
    "| Naive Bayes  | 0.8658 | 0.8766 | <u>**0.8768**</u> |\n",
    "| Logistic Regression  | 0.8908 | <u>**0.9011**</u> | 0.9008 |\n",
    "\n",
    "As can be seen in general, a higher accuracy of `0.01` is obtained by performing a `soft preprocessing` or not performing any preprocessing at all. The highest accuracy is achieved by the model trained using the `Support Vector Classifier` with an accuracy of `0.9049`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
